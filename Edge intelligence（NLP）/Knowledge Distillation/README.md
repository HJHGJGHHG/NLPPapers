# Knowledge Distillation
> BERT 用剩的 logits 不要扔，沾上鸡蛋液，裹上面包糠...

## Overview
* **Knowledge Distillation: A Survey**.  *Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao*.  (Int. J. Comput. Vis. 2021)
## Papers
* **Distilling the Knowledge in a Neural Network**.  *Geoffrey Hinton, Oriol Vinyals, Jeff Dean*.  (NIPS 2014)  [[pdf]](https://arxiv.org/pdf/1503.02531v1.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network)
* **Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**.  *Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin*.  [[pdf]](https://arxiv.org/pdf/1903.12136v1.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/DistilLSTM)  -***DistilLSTM***  [[my code]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/DistilLSTM/code)
* **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**.  *Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf*.  (NeurIPS 2019)  [[pdf]](https://arxiv.org/pdf/1910.01108v4.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/DistilBERT)  -***DistilBERT***
* **Patient Knowledge Distillation for BERT Model Compression**.  *Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu*.  (EMNLP 2019)  [[pdf]](https://arxiv.org/pdf/1908.09355.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/BERT-PKD)  -***BERT-PKD***
* **TinyBERT: Distilling BERT for Natural Language Understanding**.  *Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu*.  (ACL Findings 2020)  [[Findings pdf]](https://aclanthology.org/2020.findings-emnlp.372.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/TinyBERT)  -***TinyBERT***
* **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**.  *Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou*.  (ACL 2020)  [[ACL pdf]](https://aclanthology.org/2020.acl-main.195.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/MobileBERT)  -***MobileBERT***
* **MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers**.  *Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou*.  (NeurIPS 2020)  [[NeurIPS pdf]](https://proceedings.neurips.cc//paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/MiniLM)  -***MiniLM***
* **MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers**.  *Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu Wei*.  (ACL Findings 2021)  [[Findings pdf]](https://aclanthology.org/2021.findings-acl.188.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/MiniLMv2)  -***MiniLMv2***

## Applications

