# Knowledge Distillation
> BERT 用剩的 logits 不要扔，沾上鸡蛋液，裹上面包糠...

## Overview
* **Knowledge Distillation: A Survey**.  *Jianping Gou, Baosheng Yu, Stephen John Maybank, Dacheng Tao*.  (Int. J. Comput. Vis. 2021)
## Papers
* **Distilling the Knowledge in a Neural Network**.  *Geoffrey Hinton, Oriol Vinyals, Jeff Dean*.  (NIPS 2014)  [[pdf]](https://arxiv.org/pdf/1503.02531v1.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89/Knowledge%20Distillation/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network)
* **Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**.  *Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin*.  [[pdf]](https://arxiv.org/pdf/1903.12136v1.pdf)  [[notes]]()  -***DistilLSTM***

## Applications

