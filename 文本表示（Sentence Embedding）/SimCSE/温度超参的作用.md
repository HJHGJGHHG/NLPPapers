# 温度超参
&emsp;&emsp;主要参考：
* [CVPR2021 自监督学习论文：理解对比损失的性质以及温度系数的作用](https://zhuanlan.zhihu.com/p/357071960)  
* [Understanding the Behaviour of Contrastive Loss](https://arxiv.org/pdf/2012.09740.pdf)  
* [Contrastive Loss 中参数 τ 的理解](https://wmathor.com/index.php/archives/1581/)

&emsp;&emsp;对比学习中的损失函数一般取对比损失（InfoNCE Loss）：
$$
\mathcal{L}({x_i})=-\log \frac{e^{\frac {s_{i,i}}{\tau}}}{\sum_j e^{\frac {s_{i,j}}{\tau}}}
$$

&emsp;&emsp;直观来说，该损失函数要求第 $i$ 个样本和它另一个扩增的（正）样本之间的相似度 $s_{i,i}$ 之间尽可能大，而与其它实例（负样本）之间的相似度 $s_{i,j}$ 之间尽可能小。  
&emsp;&emsp;其实还有很多损失函数可以满足这个要求，例如下面的 loss：
$$
\mathcal{L}_{simple}({x_i})=-s_{i,i}+\lambda \sum_{j\neq i} s_{i,j}
$$
&emsp;&emsp;而实验发现这种 loss 效果远不如 InfoNCE Loss。simple loss 对所有负样本给予了相同的权重 $\frac{\partial \mathcal{L}_{simple}}{s_{i,k}}=\lambda$，而对于 InfoNCE Loss 有：
* 对正例对的梯度：$\frac{\partial \mathcal{L}(x_i)}{s_{i,k}}=-\frac{1}{\tau}\frac{\sum_{j\neq i} e^{\frac {s_{i,j}}{\tau}}}{e^{\frac {s_{i,i}}{\tau}}(e^{\frac {s_{i,i}}{\tau}}+\sum_{j\neq i} e^{\frac {s_{i,j}}{\tau}})}$
* 对负例对的梯度：$\frac{\partial \mathcal{L}(x_i)}{s_{i,k}}=\frac{1}{\tau} \frac{\frac{e^{s_{i,k}}}{\tau}}{\frac{e^{s_{i,k}}}{\tau}+\sum_{j\neq k} \frac{e^{s_{i,j}}}{\tau}},\ k\neq i$  

&emsp;&emsp;对于不同的负例而言，分母 $\frac{e^{s_{i,k}}}{\tau}+\sum_{j\neq k} \frac{e^{s_{i,j}}}{\tau}$ 均相同，那么 $s_{i,k}$ 越大，则梯度越大。也就是说，Contrastive Loss 给予了更相似（困难）负样本更大的远离该样本的梯度。  
&emsp;&emsp;我们可以把不同的负样本想像成同极点电荷在不同距离处的受力情况，距离越近的点电荷受到的库伦斥力更大，而距离越远的点电荷受到的斥力越小。对比损失中，越近的负例受到的斥力越大，具体的表现就是对应的负梯度值越大。这种性质更有利于形成在超球面均匀分布的特征。  
&emsp;&emsp;所谓困难负例即 $s_{i,k}\ge s_{i,i}$ 。我们接下来进一步分析 $\tau$ 大小的影响。  

### 1.  $\tau \to 0$
&emsp;&emsp;有：
$$
\begin{align}
&\lim_{\tau \to 0^+} -\log \frac{e^{\frac {s_{i,i}}{\tau}}}{\sum_j e^{\frac {s_{i,j}}{\tau}}}\\
=&\lim_{\tau \to 0^+} \log (1+\sum_{j\neq i} e^{\frac{1}{\tau}(s_{i,j}-s_{i,i})})\\
\approx&\lim_{\tau \to 0^+} \log (1+\sum_{s_{i,j}\ge s_{i,i},j\neq i}e^{\frac{1}{\tau}(s_{i,j}-s_{i,i})})\\
\approx&\lim_{\tau \to 0^+} \frac{1}{\tau} \max_j(s_{max}, 0)
\end{align}
$$
&emsp;&emsp;可以发现此时 Contrastive Loss 退化为***只关注最困难负例***的损失函数。

### 2. $\tau \to +\infty$
&emsp;&emsp;有：
$$
\begin{align}
&\lim_{\tau \to +\infty} -\log \frac{e^{\frac {s_{i,i}}{\tau}}}{\sum_j e^{\frac {s_{i,j}}{\tau}}}\\
=&TODO
\end{align}
$$