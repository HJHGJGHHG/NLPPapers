# Efficient Model Architectures
## Chase of the linear attention
### 1. Kernel method
* **Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel**.  *Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, Ruslan Salakhutdinov*  (EMNLP 2019)  [[pdf]](https://arxiv.org/pdf/1908.11775v4.pdf)
* **Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention**.  *Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran√ßois Fleuret*.  (ICML 2020)  [[pdf]](https://arxiv.org/pdf/2006.16236.pdf)
* **Finetuning Pretrained Transformers into RNNs**.  *Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith*.  (EMNLP 2021)  [[EMNLP pdf]](https://aclanthology.org/2021.emnlp-main.830.pdf)