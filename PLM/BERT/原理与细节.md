# BERT原理与细节
&emsp;&emsp;BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding，即使用双向Transformer的Encoder做NLU。