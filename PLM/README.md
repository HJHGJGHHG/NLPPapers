# PLM
## Introduction
The major differences between PTMs are the usages of contextual encoders, pre-training tasks, and purposes. I follow the analysis of [Qiu et al](https://arxiv.org/pdf/2003.08271.pdf) and give a taxonomy of PTMs.
## Survey
* **Pre-Trained Models: Past, Present and Future**.  *Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, Jun Zhu*.   [[pdf](https://arxiv.org/abs/2106.07139)]  
* **Pre-trained Models for Natural Language Processing: A Survey**.  *Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai & Xuanjing Huang*.  [[pdf]](https://arxiv.org/pdf/2003.08271.pdf)