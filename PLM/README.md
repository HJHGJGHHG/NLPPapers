# PLM
## Introduction
The major differences between PTMs are the usages of contextual encoders, pre-training tasks, and purposes. I follow the analysis of [Qiu et al](https://arxiv.org/pdf/2003.08271.pdf) and give a taxonomy of PTMs.
## Survey
* **Pre-Trained Models: Past, Present and Future**.  *Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, Jun Zhu*.   [[pdf](https://arxiv.org/abs/2106.07139)]  
* **Pre-trained Models for Natural Language Processing: A Survey**.  *Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai & Xuanjing Huang*.  [[pdf]](https://arxiv.org/pdf/2003.08271.pdf)  

## Basic Models & Pre-training tasks
Transformer-based models and different pre-training tasks.  
* **Attention is All you Need**.  *Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin*.  (NIPS 2017)  [[NIPS pdf]](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/PLM/Basic%20Models%20%26%20Pre-training%20tasks/Transformer)  -***Transformer***  
* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**.  *Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*.  (NAACL 2019)  [[NAACL pdf]](https://aclanthology.org/N19-1423.pdf)  [[notes]](https://github.com/HJHGJGHHG/NLPPapers/tree/main/PLM/Basic%20Models%20%26%20Pre-training%20tasks/BERT)  -***BERT***  
* **RoBERTa: A Robustly Optimized BERT Pretraining Approach**.  *Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanovs*.  [[pdf]](https://arxiv.org/pdf/1907.11692.pdf)  -***RoBERTa***  

## Computational Efficiency
See https://github.com/HJHGJGHHG/NLPPapers/tree/main/PLM/Computational%20Efficiency and https://github.com/HJHGJGHHG/NLPPapers/tree/main/Edge%20intelligence%EF%BC%88NLP%EF%BC%89.  

## Besides Transformer
Other Model Architectures such as MLPs and GAU.  
* **Transformer Quality in Linear Time**.  *Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le*.  [[pdf]](https://arxiv.org/pdf/2202.10447)  [[notes]]()  -***FLASH***

## Positional Embedding
Analysis on positional embedding.  
* **RoFormer: Enhanced Transformer with Rotary Position Embedding**.  *Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu*.  [[pdf]](https://arxiv.org/pdf/2104.09864v2.pdf)  [[notes]]()  -***RoFormer***